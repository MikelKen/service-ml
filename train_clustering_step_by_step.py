#!/usr/bin/env python3
"""
ğŸ“ ENTRENAMIENTO PASO A PASO - CLUSTERING DE CANDIDATOS
Entrena modelo de clustering no supervisado y guarda archivos .pkl
"""

import asyncio
import pandas as pd
import numpy as np
import os
from datetime import datetime
import sys
import logging

# Agregar el directorio raÃ­z al path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.config.mongodb_connection import get_collection
from app.ml.preprocessing.candidates_clustering_preprocessor import CandidatesClusteringPreprocessor
from app.ml.models.candidates_clustering_model import CandidatesClusteringModel

# Configurar logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class CandidatesClusteringTrainer:
    """Entrenador completo para clustering de candidatos"""
    
    def __init__(self):
        self.models_dir = "trained_models/clustering"
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Nombres de archivos con identificador clustering
        self.files = {
            'preprocessor': f"{self.models_dir}/candidates_clustering_preprocessor_{self.timestamp}.pkl",
            'kmeans_model': f"{self.models_dir}/candidates_clustering_kmeans_{self.timestamp}.pkl",
            'dbscan_model': f"{self.models_dir}/candidates_clustering_dbscan_{self.timestamp}.pkl",
            'data_processed': f"{self.models_dir}/candidates_clustering_data_{self.timestamp}.pkl"
        }
        
        # Crear directorio
        os.makedirs(self.models_dir, exist_ok=True)
    
    async def step_1_load_data(self) -> pd.DataFrame:
        """PASO 1: Cargar datos de MongoDB"""
        logger.info("ğŸ”¥ PASO 1: CARGANDO DATOS DE MONGODB")
        logger.info("="*60)
        
        try:
            collection = await get_collection("candidates_features")
            
            logger.info("ğŸ“Š Obteniendo datos de candidatos...")
            cursor = collection.find({})
            data = await cursor.to_list(length=None)
            
            if not data:
                raise ValueError("No se encontraron datos en candidates_features")
            
            # Convertir a DataFrame
            df = pd.DataFrame(data)
            
            logger.info(f"âœ… Datos cargados: {len(df)} candidatos")
            logger.info(f"ğŸ“‹ Columnas: {list(df.columns)}")
            
            # EstadÃ­sticas bÃ¡sicas
            logger.info("\nğŸ“ˆ ESTADÃSTICAS BÃSICAS:")
            logger.info(f"  â€¢ Total candidatos: {len(df)}")
            logger.info(f"  â€¢ Experiencia promedio: {df['anios_experiencia'].mean():.1f} aÃ±os")
            logger.info(f"  â€¢ Niveles educaciÃ³n Ãºnicos: {df['nivel_educacion'].nunique()}")
            logger.info(f"  â€¢ Candidatos con certificaciones: {df['certificaciones'].notna().sum()}")
            
            return df
            
        except Exception as e:
            logger.error(f"âŒ Error cargando datos: {e}")
            raise
    
    def step_2_preprocess_data(self, df: pd.DataFrame) -> tuple:
        """PASO 2: Preprocessar datos para clustering"""
        logger.info("\nğŸ”§ PASO 2: PREPROCESSING DE DATOS")
        logger.info("="*60)
        
        try:
            # Crear preprocessor
            preprocessor = CandidatesClusteringPreprocessor()
            
            # Preprocessar datos
            logger.info("ğŸ”„ Iniciando preprocessing...")
            X_processed = preprocessor.fit_transform(df)
            
            logger.info(f"âœ… Preprocessing completado")
            logger.info(f"ğŸ“Š Matriz procesada: {X_processed.shape}")
            logger.info(f"ğŸ·ï¸ Features extraÃ­das: {len(preprocessor.feature_names)}")
            
            # Guardar preprocessor
            preprocessor.save_preprocessor(self.files['preprocessor'])
            
            # Guardar datos procesados
            processed_data = {
                'X_processed': X_processed,
                'feature_names': preprocessor.feature_names,
                'original_data': df,
                'preprocessing_date': datetime.now().isoformat()
            }
            
            import pickle
            with open(self.files['data_processed'], 'wb') as f:
                pickle.dump(processed_data, f)
            
            logger.info(f"ğŸ’¾ Datos procesados guardados en: {self.files['data_processed']}")
            
            return X_processed, preprocessor
            
        except Exception as e:
            logger.error(f"âŒ Error en preprocessing: {e}")
            raise
    
    def step_3_train_kmeans(self, X: np.ndarray, feature_names: list) -> CandidatesClusteringModel:
        """PASO 3: Entrenar modelo K-Means"""
        logger.info("\nğŸ¯ PASO 3: ENTRENAMIENTO K-MEANS")
        logger.info("="*60)
        
        try:
            # Crear modelo K-Means
            kmeans_model = CandidatesClusteringModel(algorithm='kmeans')
            
            # Entrenar con bÃºsqueda de clusters Ã³ptimos
            logger.info("ğŸ” Buscando nÃºmero Ã³ptimo de clusters...")
            kmeans_model.fit(X, find_optimal=True)
            
            # Obtener perfiles de clusters
            logger.info("ğŸ“Š Generando perfiles de clusters...")
            cluster_profiles = kmeans_model.get_cluster_profiles(X, feature_names)
            
            # Mostrar resultados
            logger.info("\nğŸ“ˆ RESULTADOS K-MEANS:")
            summary = kmeans_model.get_cluster_summary()
            logger.info(f"  ğŸ¯ Clusters encontrados: {summary['n_clusters_found']}")
            logger.info(f"  ğŸ“Š Silhouette Score: {summary['metrics'].get('silhouette_score', 'N/A')}")
            
            for cluster_id, profile in cluster_profiles.items():
                logger.info(f"  ğŸ·ï¸ Cluster {cluster_id}: {profile['size']} candidatos ({profile['percentage']:.1f}%)")
            
            # Guardar modelo
            kmeans_model.save_model(self.files['kmeans_model'])
            
            return kmeans_model
            
        except Exception as e:
            logger.error(f"âŒ Error entrenando K-Means: {e}")
            raise
    
    def step_4_train_dbscan(self, X: np.ndarray, feature_names: list) -> CandidatesClusteringModel:
        """PASO 4: Entrenar modelo DBSCAN"""
        logger.info("\nğŸŒ PASO 4: ENTRENAMIENTO DBSCAN")
        logger.info("="*60)
        
        try:
            # Crear modelo DBSCAN
            dbscan_model = CandidatesClusteringModel(algorithm='dbscan')
            
            # Entrenar
            logger.info("ğŸ”„ Entrenando DBSCAN...")
            dbscan_model.fit(X, find_optimal=False)
            
            # Obtener perfiles
            cluster_profiles = dbscan_model.get_cluster_profiles(X, feature_names)
            
            # Mostrar resultados
            logger.info("\nğŸ“ˆ RESULTADOS DBSCAN:")
            summary = dbscan_model.get_cluster_summary()
            logger.info(f"  ğŸ¯ Clusters encontrados: {summary['n_clusters_found']}")
            logger.info(f"  âš ï¸ Outliers: {summary['n_outliers']}")
            
            for cluster_id, profile in cluster_profiles.items():
                logger.info(f"  ğŸ·ï¸ Cluster {cluster_id}: {profile['size']} candidatos ({profile['percentage']:.1f}%)")
            
            # Guardar modelo
            dbscan_model.save_model(self.files['dbscan_model'])
            
            return dbscan_model
            
        except Exception as e:
            logger.error(f"âŒ Error entrenando DBSCAN: {e}")
            raise
    
    def step_5_generate_summary(self, kmeans_model: CandidatesClusteringModel, 
                               dbscan_model: CandidatesClusteringModel,
                               df: pd.DataFrame):
        """PASO 5: Generar resumen final"""
        logger.info("\nğŸ“Š PASO 5: RESUMEN FINAL")
        logger.info("="*60)
        
        # Resumen de entrenamiento
        summary = {
            'training_date': datetime.now().isoformat(),
            'dataset_size': len(df),
            'models_trained': ['kmeans', 'dbscan'],
            'files_generated': self.files,
            'kmeans_results': kmeans_model.get_cluster_summary(),
            'dbscan_results': dbscan_model.get_cluster_summary()
        }
        
        # Guardar resumen
        summary_file = f"{self.models_dir}/training_summary_{self.timestamp}.json"
        import json
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        logger.info("âœ… ENTRENAMIENTO COMPLETADO EXITOSAMENTE")
        logger.info("\nğŸ“ ARCHIVOS GENERADOS:")
        for name, filepath in self.files.items():
            if os.path.exists(filepath):
                size_mb = os.path.getsize(filepath) / (1024 * 1024)
                logger.info(f"  âœ… {name}: {filepath} ({size_mb:.1f} MB)")
        
        logger.info(f"\nğŸ“Š RESUMEN:")
        logger.info(f"  ğŸ“ˆ Dataset: {len(df)} candidatos procesados")
        logger.info(f"  ğŸ¯ K-Means: {summary['kmeans_results']['n_clusters_found']} clusters")
        logger.info(f"  ğŸŒ DBSCAN: {summary['dbscan_results']['n_clusters_found']} clusters")
        logger.info(f"  ğŸ’¾ Archivos: {len([f for f in self.files.values() if os.path.exists(f)])} generados")
        
        return summary
    
    async def train_complete_pipeline(self):
        """Ejecuta el pipeline completo de entrenamiento"""
        logger.info("ğŸš€ INICIANDO ENTRENAMIENTO COMPLETO DE CLUSTERING")
        logger.info("ğŸ¯ Objetivo: Agrupar candidatos por similitud de perfil")
        logger.info("â±ï¸ Timestamp: " + self.timestamp)
        logger.info("="*80)
        
        try:
            # PASO 1: Cargar datos
            df = await self.step_1_load_data()
            
            # PASO 2: Preprocessar
            X_processed, preprocessor = self.step_2_preprocess_data(df)
            
            # PASO 3: Entrenar K-Means
            kmeans_model = self.step_3_train_kmeans(X_processed, preprocessor.feature_names)
            
            # PASO 4: Entrenar DBSCAN
            dbscan_model = self.step_4_train_dbscan(X_processed, preprocessor.feature_names)
            
            # PASO 5: Resumen final
            summary = self.step_5_generate_summary(kmeans_model, dbscan_model, df)
            
            logger.info("\nğŸ‰ Â¡ENTRENAMIENTO COMPLETADO CON Ã‰XITO!")
            logger.info("ğŸš€ Siguiente paso: Implementar GraphQL para consultas")
            
            return summary
            
        except Exception as e:
            logger.error(f"ğŸ’¥ Error en el entrenamiento: {e}")
            raise

async def main():
    """FunciÃ³n principal"""
    trainer = CandidatesClusteringTrainer()
    await trainer.train_complete_pipeline()

if __name__ == "__main__":
    asyncio.run(main())